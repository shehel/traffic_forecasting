{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T12:10:56.469161Z",
     "start_time": "2022-01-30T12:10:49.939834Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.common.h5_util import load_h5_file\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_high = load_h5_file('.h5')\n",
    "plt.imsave('map_high.png', map_high)\n",
    "dx= 30*10\n",
    "dy= 30*10\n",
    "submapx1 = 471*10\n",
    "submapx2 = 471*10+dx\n",
    "submapy1 = 138*10\n",
    "submapy2 = 138*10+dy\n",
    "plt.imshow(map_high[submapx1:submapx2, submapy1:submapy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T12:11:03.114237Z",
     "start_time": "2022-01-30T12:11:02.997652Z"
    }
   },
   "outputs": [],
   "source": [
    "map_static = load_h5_file('../data/raw/default/ANTWERP/ANTWERP_static.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('antwerp.png', map_static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(map_static[0][217:219,206:207])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "print (map_static.shape)\n",
    "plt.imshow(map_static[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import os\n",
    "from matplotlib.image import imread\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input single channel\n",
    "x = map_static[0].astype(np.float32)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = imread(os.path.join(\"/home/shehel/\",\"Downloads\",\"cameraman.png\"))\n",
    "A = np.interp(A, (A.min(), A.max()), (0, 255))\n",
    "A = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "w = 'db7'\n",
    "coeffs = pywt.wavedec2(A,wavelet=w, level=n)\n",
    "\n",
    "# normalizing coefficient array\n",
    "coeffs[0] /= np.abs(coeffs[0]).max()\n",
    "for detail_level in range(n):\n",
    "    coeffs[detail_level+1] = [d/np.abs(d).max() for d in coeffs[detail_level + 1]]\n",
    "#coeffs[1][-1]=np.zeros((128,128))\n",
    "#coeffs[-2]=(np.zeros((64,64)),np.zeros((64,64)),np.zeros((64,64)))\n",
    "#coeffs[-1]=(np.zeros((128,128)),np.zeros((128,128)),np.zeros((128,128)))\n",
    "#coeffs[-2]=(np.zeros((128,128)),np.zeros((128,128)),np.zeros((128,128)))\n",
    "arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
    "plt.imshow(arr, vmin=-0.25, vmax=0.75)\n",
    "plt.rcParams['figure.figsize'] = [16,16]\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coeffs = pywt.wavedec2(A,wavelet='db1', level=2)\n",
    "#coeffs[-1]=(coeffs[-1][0],coeffs[-1][1],np.zeros((128,128)))\n",
    "coeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
    "coeffs_filt = pywt.array_to_coeffs(coeff_arr, coeff_slices, output_format='wavedec2')\n",
    "\n",
    "recon = pywt.waverec2(coeffs_filt, wavelet=w)\n",
    "#plt.figure()\n",
    "plt.imshow(recon,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.rcParams['figure.figsize'] = [16,16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'db1'\n",
    "coeffs = pywt.wavedec2(A,wavelet=w, level=1)\n",
    "dim1, dim2 = coeffs[-1][0].shape \n",
    "\n",
    "#coeffs[-2]=(np.zeros((64,64)),np.zeros((64,64)),np.zeros((64,64)))\n",
    "#coeffs[-1]=(np.zeros((256,256)),np.zeros((256,256)),np.zeros((128,128)))\n",
    "coeffs[-1]=(np.zeros((dim1,dim2)),np.zeros((dim1,dim2)),np.zeros((dim1,dim2)))\n",
    "#coeffs[-1]=(coeffs[-1][0],coeffs[-1][1],np.zeros((dim1,dim2)))\n",
    "coeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
    "coeffs_filt = pywt.array_to_coeffs(coeff_arr, coeff_slices, output_format='wavedec2')\n",
    "\n",
    "recon = pywt.waverec2(coeffs_filt, wavelet=w)\n",
    "plt.figure()\n",
    "plt.imshow(recon)\n",
    "#plt.axis('off')\n",
    "#pt.rcParams['figure.figsize'] = [16,16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(recon[:495], A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(A.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A - recon[:495], cmap='gray')\n",
    "max((A - recon[:495]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(coeff_arr, vmin=-0.25, vmax=0.75)\n",
    "plt.rcParams['figure.figsize'] = [16,16]\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Wavelet Compression\n",
    "n = 4\n",
    "w = 'db1'\n",
    "coeffs = pywt.wavedec2(A,wavelet=w,level=n)\n",
    "\n",
    "coeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
    "\n",
    "Csort = np.sort(np.abs(coeff_arr.reshape(-1)))\n",
    "\n",
    "for keep in (0.8,0.5):\n",
    "    thresh = Csort[int(np.floor((1-keep)*len(Csort)))]\n",
    "    ind = np.abs(coeff_arr) > thresh\n",
    "    Cfilt = coeff_arr * ind # Threshold small indices\n",
    "    \n",
    "    coeffs_filt = pywt.array_to_coeffs(Cfilt,coeff_slices,output_format='wavedec2')\n",
    "    \n",
    "    # Plot reconstruction\n",
    "    Arecon = pywt.waverec2(coeffs_filt,wavelet=w)\n",
    "    plt.figure()\n",
    "    plt.imshow(Arecon,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('keep = ' + str(keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(Arecon[:495], A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(recon.round()[:495, :], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "from matplotlib import pyplot as plt\n",
    "from pywt._doc_utils import wavedec2_keys, draw_2d_wp_basis\n",
    "\n",
    "x = map_static[0].astype(np.float32)\n",
    "shape = x.shape\n",
    "\n",
    "max_lev = 3       # how many levels of decomposition to draw\n",
    "label_levels = 3  # how many levels to explicitly label on the plots\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=[28, 16])\n",
    "for level in range(0, max_lev + 1):\n",
    "    if level == 0:\n",
    "        # show the original image before decomposition\n",
    "        axes[0, 0].set_axis_off()\n",
    "        axes[1, 0].imshow(x, cmap=plt.cm.gray)\n",
    "        axes[1, 0].set_title('Image')\n",
    "        axes[1, 0].set_axis_off()\n",
    "        continue\n",
    "\n",
    "    # plot subband boundaries of a standard DWT basis\n",
    "    draw_2d_wp_basis(shape, wavedec2_keys(level), ax=axes[0, level],\n",
    "                     label_levels=label_levels)\n",
    "    axes[0, level].set_title('{} level\\ndecomposition'.format(level))\n",
    "\n",
    "    # compute the 2D DWT\n",
    "    c = pywt.wavedec2(x, 'db2', mode='periodization', level=level)\n",
    "    # normalize each coefficient array independently for better visibility\n",
    "    c[0] /= np.abs(c[0]).max()\n",
    "    for detail_level in range(level):\n",
    "        c[detail_level + 1] = [d/np.abs(d).max() for d in c[detail_level + 1]]\n",
    "    # show the normalized coefficients\n",
    "    arr, slices = pywt.coeffs_to_array(c)\n",
    "    axes[1, level].imshow(arr, cmap=plt.cm.gray)\n",
    "    axes[1, level].set_title('Coefficients\\n({} level)'.format(level))\n",
    "    axes[1, level].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T13:00:14.209852Z",
     "start_time": "2022-01-30T13:00:13.942882Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "print (map_static.shape)\n",
    "plt.imshow(map_static[0, ::4, ::4])\n",
    "print (map_static[0, ::4, ::4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "print (map_static.shape)\n",
    "plt.imshow(map_static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('map_low.png',map_static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx= 24\n",
    "dy= 30\n",
    "submapx1 = 461\n",
    "submapx2 = 461+dx\n",
    "submapy1 = 138\n",
    "submapy2 = 138+dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_static[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y axes reversed in gimp, \n",
    "#in google maps, had to rotate 90 clockwise to align \n",
    "plt.imshow(map_static[0][submapx1:submapx2, submapy1:submapy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = sorted(glob.glob(\"MOSCOW/training/*.h5\", recursive=True))\n",
    "print(f\"Found {len(training_files)} training files\")\n",
    "num_files = len(training_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = np.zeros(shape=(num_files,288, 2, 8))\n",
    "for idx,file in tqdm((enumerate(training_files[:1]))):\n",
    "    dataset_raw[idx,:,0] = load_h5_file(file)[:,483, 159,:]\n",
    "    dataset_raw[idx,:,1] = load_h5_file(file)[:,378, 166,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "  #\"\"\" x == an array of data. N == number of samples per average \"\"\"\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_h5_file(file).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "ax1.xcorr(running_mean(dataset_raw[0,:,0,2],6), running_mean(dataset_raw[0,:,1,2],6), usevlines=True, maxlags=200, normed=True, lw=2)\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(running_mean(dataset_raw[0,:,1,2],6),running_mean(dataset_raw[0,:,0,4],6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_mean(dataset_raw[0,:,1,0],3))\n",
    "plt.plot(running_mean(dataset_raw[0,:,1,2],3))\n",
    "plt.plot(running_mean(dataset_raw[0,:,1,4],3))\n",
    "plt.plot(running_mean(dataset_raw[0,:,1,6],3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for i in range(0,8,2):\n",
    "    plt.plot(running_mean(dataset_raw[0,:,1,i],3))\n",
    "\n",
    "plt.plot(running_mean(dataset_raw[0,:,0,4],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30, 109 is very close to 5 min mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files=len(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = np.zeros(shape=(num_files,288, 495,436, 8))\n",
    "for idx,file in tqdm((enumerate(training_files[:num_files]))):\n",
    "    dataset_raw[idx] = load_h5_file(file)[:,:, :,:]\n",
    "    dataset_raw[idx] = load_h5_file(file)[:,:, :,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(dataset_raw, axis=0)\n",
    "dataset_detrend = dataset_raw-np.reshape(means, (1, 288, 4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset_raw[:,:,0,4].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = np.zeros(shape=(num_files,288, 4, 8))\n",
    "for idx,file in tqdm((enumerate(training_files[:num_files]))):\n",
    "    loaded_file = load_h5_file(file)\n",
    "    dataset_raw[idx,:,0] = loaded_file[:,123, 61,:]\n",
    "    dataset_raw[idx,:,2] = loaded_file[:,47, 100,:]\n",
    "    dataset_raw[idx,:,1] = loaded_file[:,86, 80,:]\n",
    "    dataset_raw[idx,:,3] = loaded_file[:,14, 125,:]\n",
    "\n",
    "#for i in range(0,8,2):\n",
    "#    plt.plot(running_mean(dataset_raw[0,:,0,i], 6))\n",
    "\n",
    "#plt.plot(running_mean(dataset_raw[0,:,0,4],3))\n",
    "#plt.plot(running_mean(dataset_raw[0,:,1,4],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(123,61), (86,80), (47,100), (30, 109),(14,125), (388,307)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(dataset_detrend[0,:,x,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "x = 0\n",
    "x1 = 1\n",
    "ax1.xcorr(running_mean(dataset_detrend[0,:,x,4],3), running_mean(dataset_detrend[0,:,x1,4],3), usevlines=True, maxlags=200, normed=True, lw=2, label=str(x1))\n",
    "ax1.legend()\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "x,y = points[0]\n",
    "x2,y2 = points[-2]\n",
    "ax1.xcorr(running_mean(dataset_raw[0,:,x,y,4],3), running_mean(dataset_raw[0,:,x2,y2,4],3), usevlines=True, maxlags=200, normed=True, lw=2)\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_mean(dataset_raw[0,:,x2,y2,4],3))\n",
    "plt.plot(running_mean(dataset_raw[0,:,x,y,4],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in points:\n",
    "    plt.plot(running_mean(dataset_raw[0,:,x,y,4], 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,8,2):\n",
    "    plt.plot(running_mean(dataset_raw[0,:,1,i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = np.zeros(shape=(num_files,288, dx, dy, 8))\n",
    "for idx,file in tqdm((enumerate(training_files[:num_files]))):\n",
    "    dataset_raw[idx] = load_h5_file(file)[:,submapx1:submapx2, submapy1:submapy2,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hf = h5py.File('dataset_raw.h5', 'r')\n",
    "#hf.create_dataset('ds1', data=dataset_raw)\n",
    "\n",
    "n1 = hf.get('ds1')\n",
    "dataset_raw = np.array(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset_raw[0, :,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset_detrend[0,:,0,4])\n",
    "plt.plot(dataset_raw[0, :, 0,4])\n",
    "plt.plot(dataset1[:288,x,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "x = 0\n",
    "x1 = 3\n",
    "ax1.xcorr(running_mean(dataset_detrend[0,:,x,4],3), running_mean(dataset_detrend[0,:,x1,4],3), usevlines=True, maxlags=200, normed=True, lw=2, label=str(x1))\n",
    "ax1.legend()\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1)\n",
    "x = 0\n",
    "x1 = 1\n",
    "ax1.xcorr(running_mean(dataset1[:288,x,2],3), running_mean(dataset1[:288,x1,2],3), usevlines=True, maxlags=200, normed=True, lw=2, label=str(x1))\n",
    "ax1.legend()\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with user-defined points\n",
    "dataset1 = dataset_raw-np.mean(dataset_raw, axis=0)\n",
    "dataset1 = np.reshape(dataset1, (num_files*288, 4,8))\n",
    "\n",
    "\n",
    "#means = np.mean(dataset_raw, axis=0)\n",
    "#dataset_detrend = dataset_raw-np.reshape(means, (1, 288, 4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = np.reshape(dataset_raw, (num_files*288, dx, dy,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset_raw-np.mean(dataset_raw, axis=0)\n",
    "dataset1 = np.reshape(dataset1, (num_files*288, dx, dy,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "days = 10\n",
    "pixels = [[0,5],[0,15],[9,14],[13,13],[23,10],[23,11]]\n",
    "pixels = [[0,5],[9,14],[23,11]]\n",
    "\n",
    "for x,y in pixels:\n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2])\n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2][x,y])\n",
    "    plt.plot(moving_avg(dataset1[:1000,x,y,0], 2), label=str(x)+str(y))\n",
    "    plt.plot(moving_avg(dataset1[:1000,x,y,2], 2), label=str(x)+str(y), linestyle='dashed')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_avg(dataset1[:1000,9,14,2], 6))\n",
    "plt.plot(moving_avg(dataset1[:1000,23,10,2], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(2, 1, sharex=True)\n",
    "ax1.xcorr(moving_avg(dataset1[:1000,9,14,2], 6),moving_avg(dataset1[:1000,23,10,2], 6), usevlines=False, maxlags=990, normed=True, lw=1)\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(moving_avg(dataset1[:1000,9,14,2], 6),moving_avg(dataset1[:1000,23,10,2], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined points\n",
    "history = 3\n",
    "step_ahead = 0\n",
    "target_channel = 4 # South \n",
    "pixels = [3,0]\n",
    "target_pixel = 3\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(len(dataset1)-(history+step_ahead)):\n",
    "    sample_x = []\n",
    "    for x in pixels:\n",
    "        sample_x.extend(list(dataset1[i:i+history,x,target_channel]))\n",
    "      \n",
    "    Xs.append(sample_x)\n",
    "    ys.append(dataset1[i+history+step_ahead, target_pixel, target_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = 3\n",
    "step_ahead = 1\n",
    "target_channel = 2 # South \n",
    "pixels = [[13,13]]\n",
    "target_pixel = [13,13]\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(len(dataset1)-(history+step_ahead)):\n",
    "    sample_x = []\n",
    "    for x,y in pixels:\n",
    "        sample_x.extend(list(dataset1[i:i+history,x,y,target_channel]))\n",
    "      \n",
    "    Xs.append(sample_x)\n",
    "    ys.append(dataset1[i+history+step_ahead,target_pixel[0], target_pixel[1], target_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = 6\n",
    "step_ahead = 1\n",
    "target_channel = 2 # South \n",
    "pixels = [[13,13]]\n",
    "target_pixel = [13,13]\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(len(dataset1)-(history+step_ahead)):\n",
    "    sample_x = []\n",
    "    for x,y in pixels:\n",
    "        sample_x.extend(list(dataset1[i:i+history,x,y,target_channel]))\n",
    "      \n",
    "    Xs.append(sample_x)\n",
    "    ys.append(dataset1[i+history+step_ahead,target_pixel[0], target_pixel[1], target_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1[:14,target_pixel[0],target_pixel[1],target_channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "\n",
    "for x in Xs[:1]:\n",
    "    for tr, y in zip(ys,range(0,36,6)):\n",
    "        \n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2])\n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2][x,y])\n",
    "        plt.plot(x[y:y+6])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "reg1 = LinearRegression().fit(train_x.cpu(), train_y.cpu())\n",
    "#reg2 = MLPRegressor(random_state=1, max_iter=1000).fit(train_x.cpu(), train_y.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessRegressor(kernel=kernel,\n",
    "         random_state=0).fit(train_x.cpu(), train_y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsg = gpc.predict(test_x.cpu())\n",
    "\n",
    "plt.plot(test_y.cpu())\n",
    "plt.plot(predsg, label='gpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = reg1.predict(test_x.cpu())\n",
    "#preds2 = reg2.predict(test_x.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_y.cpu()[:288])\n",
    "plt.plot(preds1[:288], label='LR')\n",
    "#plt.plot(preds1[:288], label='LR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(torch.tensor(preds1) - test_y.cpu()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP\n",
    "Ref https://docs.gpytorch.ai/en/v1.1.1/examples/02_Scalable_Exact_GPs/Simple_GP_Regression_CUDA.html#Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(Xs[:-1000], dtype=torch.float32)\n",
    "train_y = torch.tensor(ys[:-1000], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(Xs[-1000:], dtype=torch.float32)\n",
    "#test_x = test_x.cuda()\n",
    "test_y = torch.tensor(ys[-1000:], dtype=torch.float32)\n",
    "#test_y = test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralMixtureGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(SpectralMixtureGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3, ard_num_dims=6).cuda()\n",
    "        #self.covar_module.initialize_from_data(train_x, train_y)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = SpectralMixtureGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.cuda()\n",
    "train_y = train_y.cuda()\n",
    "model = model.cuda()\n",
    "likelihood = likelihood.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "training_iter = 100\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "#means = torch.tensor([0.])\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Make predictions\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu().numpy()\n",
    "        #means = torch.cat([means, preds.mean.cpu()])\n",
    "#means = means[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_y.cpu())\n",
    "plt.plot(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x[:40000], train_y[:40000])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(train_x[40000:], train_y[40000:])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class SpectralMixtureGPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(SpectralMixtureGPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=6).cuda()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module.initialize_from_data(train_x, train_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = train_x[:5000, :]\n",
    "model = SpectralMixtureGPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =  1\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        print (\"hello\")\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "        print (means)\n",
    "means = means[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - train_y[40000:].cpu()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "\n",
    "if not smoke_test and not os.path.isfile('../elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', '../elevators.mat')\n",
    "\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('../elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_y[40000:].cpu())\n",
    "plt.plot(means[:288])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "days = 10\n",
    "for x in range(2,3):\n",
    "    for y in [0,7, 15,25, 29]:\n",
    "        x\n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2])\n",
    "        #print (map_static[submapx1:submapx2, submapy1:submapy2][x,y])\n",
    "        plt.plot(moving_avg(dataset1[:288*days,x,y,6], 12), label=str(x)+str(y))\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(dataset_raw, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.plot(moving_avg(dataset1[:288*3,x,0,6], 6), label=str(x)+str(y))\n",
    "plt.plot(moving_avg(dataset1[:288*3,x,29,6]*6, 6), label=str(x)+str(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "SAMPLE_RATE=288\n",
    "DURATION = 3\n",
    "# Number of samples in normalized_tone\n",
    "N = SAMPLE_RATE * DURATION\n",
    "\n",
    "yf = rfft(dataset1[:288*3,2,29,6])\n",
    "xf = rfftfreq(N, 1 / SAMPLE_RATE)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "plt.plot(xf, np.abs(yf))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "SAMPLE_RATE=288\n",
    "DURATION = num_files\n",
    "# Number of samples in normalized_tone\n",
    "N = SAMPLE_RATE * DURATION\n",
    "\n",
    "yf = rfft(dataset1[:288*num_files,2,0,6])\n",
    "xf = rfftfreq(N, 1 / SAMPLE_RATE)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "plt.plot(xf, np.abs(yf))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mogptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-02'\n",
    "end_date = '2020-01-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.reshape(dataset1, (num_files*288, 4, 14,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:288*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = pd.DataFrame(dataset[:,2,0,:])\n",
    "second = pd.DataFrame(dataset[:,2,7,:])\n",
    "third = pd.DataFrame(dataset[:,2,15,:])\n",
    "fourth = pd.DataFrame(dataset[:,2,25,:])\n",
    "fifth = pd.DataFrame(dataset[:,2,29,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bramblemet = pd.read_csv('data/bramblemet/bramblemet.csv.gz', compression='gzip', low_memory=False)\n",
    "first['Date'] = pd.date_range(start=start_date, end=end_date, freq='5t')[:-1]\n",
    "second['Date'] = pd.date_range(start=start_date, end=end_date, freq='5t')[:-1]\n",
    "third['Date'] = pd.date_range(start=start_date, end=end_date, freq='5t')[:-1]\n",
    "fourth['Date'] = pd.date_range(start=start_date, end=end_date, freq='5t')[:-1]\n",
    "fifth['Date'] = pd.date_range(start=start_date, end=end_date, freq='5t')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk = mogptk.DataSet(\n",
    "    mogptk.LoadDataFrame(first, x_col='Date', y_col=6, name='1'),\n",
    "    mogptk.LoadDataFrame(second, x_col='Date', y_col=6, name='2'),\n",
    "        mogptk.LoadDataFrame(third, x_col='Date', y_col=6, name='3'),\n",
    "        mogptk.LoadDataFrame(fourth, x_col='Date', y_col=6, name='4'),\n",
    "        mogptk.LoadDataFrame(fifth, x_col='Date', y_col=6, name='5'),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(ds_mogptk):\n",
    "    #data.transform(mogptk.TransformDetrend)\n",
    "    data.remove_randomly(pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk['1'].remove_range(start='2020-01-04 12:00:00', end='2020-01-05')\n",
    "ds_mogptk['2'].remove_range(start='2020-01-04 12:00:00', end='2020-01-05')\n",
    "ds_mogptk['3'].remove_range(start='2020-01-04 12:00:00', end='2020-01-05')\n",
    "ds_mogptk['4'].remove_range(start='2020-01-04 12:00:00', end='2020-01-05')\n",
    "ds_mogptk['5'].remove_range(start='2020-01-04 12:00:00', end='2020-01-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(transformed=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot_spectrum(per='day', maxfreq=10, transformed=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Adam'\n",
    "lr = 0.1\n",
    "iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosm = mogptk.MOSM(ds_mogptk, Q=10)\n",
    "mosm.init_parameters(method='LS')\n",
    "mosm.train(method=method, lr=lr, iters=iters, verbose=True)\n",
    "mosm.predict();\n",
    "mosm.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(figsize=(25,25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(figsize=(25,25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(figsize=(25,25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(figsize=(25,25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mogptk.plot(figsize=(25,25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
